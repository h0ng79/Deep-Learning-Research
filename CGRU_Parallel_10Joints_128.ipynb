{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bLxNqvNpHP7",
        "outputId": "80fa2cd4-31b4-4b6d-8526-cf2d3899488c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path =  '/content/drive/MyDrive/Research MU/'"
      ],
      "metadata": {
        "id": "xtpudj1z5rZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPF0Vv4pamc"
      },
      "source": [
        "import pandas\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import CSVLogger\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import multiprocessing as mp\n",
        "import timeit\n",
        "\n",
        "# import tensorflow as tf\n",
        "# physical_devices = tf.config.list_physical_devices('GPU') \n",
        "# for device in physical_devices:\n",
        "#     tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "\n",
        "\n",
        "def CGRUmodel_dj1(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj1.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj1.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj1.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj1.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj2(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj2.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj2.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj2.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj2.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj3(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj3.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj3.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj3.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj3.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj4(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj4.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj4.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj4.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj4.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj5(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj5.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj5.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj5.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj5.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "\n",
        "def CGRUmodel_dj6(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj6.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj6.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj6.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj6.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj7(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj7.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj7.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj7.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj7.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj8(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj8.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj8.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj8.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj8.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj9(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj9.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj9.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj9.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj9.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj10(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj10.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj10.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj10.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj10.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj11(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj11.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj11.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj11.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj11.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj12(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj12.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj12.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj12.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj12.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj13(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj13.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj13.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj13.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj13.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj14(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj14.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj14.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj14.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj14.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "\n",
        "def CGRUmodel_dj15(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj15.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj15.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj15.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj15.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj16(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj16.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj16.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj16.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj16.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj17(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj17.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj17.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj17.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj17.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj18(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj18.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj18.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj18.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj18.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "\n",
        "def CGRUmodel_dj19(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj19.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj19.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj19.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj19.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n",
        "\n",
        "def CGRUmodel_dj20(input_node, n_timesteps, output_node, x_train, y_train,x_test, y_test):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu', input_shape=(input_node, n_timesteps)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=7, activation='elu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(GRU(units=128, recurrent_dropout=0.2, activation='elu'))\n",
        "    model.add(Dense(128, activation='elu'))\n",
        "    model.add(Dense(output_node))\n",
        "    model.summary()\n",
        "    json_string = model.to_json()\n",
        "    open('/content/drive/My Drive/Parallel_10Joints_128/model_dj20.json', 'w').write(json_string)\n",
        "    model.compile(loss='mse', optimizer='adamax', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    csv_logger = CSVLogger('/content/drive/My Drive/Parallel_10Joints_128/training_history_dj20.csv')\n",
        "    history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test),\n",
        "                                batch_size=128, verbose=2, callbacks=[csv_logger])\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    model.save_weights('/content/drive/My Drive/Parallel_10Joints_128/target_weight_dj20.h5', overwrite=True)\n",
        "    pred_train = model.predict(x_train)\n",
        "    pred_test = model.predict(x_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    mse_train = mean_squared_error(pred_train, y_train)\n",
        "    mse_test = mean_squared_error(pred_test, y_test)\n",
        "    rmse_train = mean_squared_error(pred_train, y_train, squared=False)\n",
        "    rmse_test = mean_squared_error(pred_test, y_test, squared=False)\n",
        "    mae_train = mean_absolute_error(pred_train, y_train)\n",
        "    mae_test = mean_absolute_error(pred_test, y_test)\n",
        "    r2_train= r2_score(y_train, pred_train)\n",
        "    r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Parallel_10Joints_128/MSE_Computingtime_dj20.txt\", \"w\") as text_file:\n",
        "        print('mse_train: ' + str(mse_train), file=text_file)\n",
        "        print('mse_test : ' + str(mse_test), file=text_file)\n",
        "        print('rmse_train: ' + str(rmse_train), file=text_file)\n",
        "        print('rmse_test: ' + str(rmse_test), file=text_file)\n",
        "        print('mae_train : ' + str(mae_train), file=text_file)\n",
        "        print('mae_test: ' + str(mae_test), file=text_file)\n",
        "        print('r2_train:' + str(r2_train), file=text_file)\n",
        "        print('r2_test:' + str(r2_test), file=text_file)\n",
        "        print('scores: ' + str(scores), file=text_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    dataframe_dj1 = pandas.read_csv(path+'DataJoin1.csv')\n",
        "    dataset_dj1 = dataframe_dj1.values\n",
        "\n",
        "    data_dj1 = dataset_dj1[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj1 = scaler.fit_transform(data_dj1)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj1 = data_dj1[:nb_train, 0:30]\n",
        "    x_test_dj1 = data_dj1[nb_train:nb_set, 0:30]\n",
        "    y_train_dj1 = dataset_dj1[:nb_train, 30:31]\n",
        "    y_test_dj1 = dataset_dj1[nb_train:nb_set, 30:31]\n",
        "    \n",
        "    x_train_dj1 = x_train_dj1.reshape(x_train_dj1.shape[0], x_train_dj1.shape[1], 1)\n",
        "    x_test_dj1 = x_test_dj1.reshape(x_test_dj1.shape[0], x_test_dj1.shape[1], 1)\n",
        "    input_node_dj1 = x_train_dj1.shape[1]\n",
        "    output_node_dj1 = y_train_dj1.shape[1]\n",
        "    n_timesteps_dj1 = x_train_dj1.shape[2]\n",
        "\n",
        "    dataframe_dj2 = pandas.read_csv(path+'DataJoin2.csv')\n",
        "    dataset_dj2 = dataframe_dj2.values\n",
        "    data_dj2 = dataset_dj2[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj2 = scaler.fit_transform(data_dj2)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj2 = data_dj2[:nb_train, 0:30]\n",
        "    x_test_dj2 = data_dj2[nb_train:nb_set, 0:30]\n",
        "    y_train_dj2 = dataset_dj2[:nb_train, 30:31]\n",
        "    y_test_dj2 = dataset_dj2[nb_train:nb_set, 30:31]\n",
        "    x_train_dj2 = x_train_dj2.reshape(x_train_dj2.shape[0], x_train_dj2.shape[1], 1)\n",
        "    x_test_dj2 = x_test_dj2.reshape(x_test_dj2.shape[0], x_test_dj2.shape[1], 1)\n",
        "    input_node_dj2 = x_train_dj2.shape[1]\n",
        "    output_node_dj2 = y_train_dj2.shape[1]\n",
        "    n_timesteps_dj2 = x_train_dj2.shape[2]\n",
        "\n",
        "    dataframe_dj3 = pandas.read_csv(path+'DataJoin3.csv')\n",
        "    dataset_dj3 = dataframe_dj3.values\n",
        "    data_dj3 = dataset_dj3[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj3 = scaler.fit_transform(data_dj3)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj3 = data_dj3[:nb_train, 0:30]\n",
        "    x_test_dj3 = data_dj3[nb_train:nb_set, 0:30]\n",
        "    y_train_dj3 = dataset_dj3[:nb_train, 30:31]\n",
        "    y_test_dj3 = dataset_dj3[nb_train:nb_set, 30:31]\n",
        "    x_train_dj3 = x_train_dj3.reshape(x_train_dj3.shape[0], x_train_dj3.shape[1], 1)\n",
        "    x_test_dj3 = x_test_dj3.reshape(x_test_dj3.shape[0], x_test_dj3.shape[1], 1)\n",
        "    input_node_dj3 = x_train_dj3.shape[1]\n",
        "    output_node_dj3 = y_train_dj3.shape[1]\n",
        "    n_timesteps_dj3 = x_train_dj3.shape[2]\n",
        "\n",
        "    dataframe_dj4 = pandas.read_csv(path+'DataJoin4.csv')\n",
        "    dataset_dj4 = dataframe_dj4.values\n",
        "    data_dj4 = dataset_dj4[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj4 = scaler.fit_transform(data_dj4)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj4 = data_dj4[:nb_train, 0:30]\n",
        "    x_test_dj4 = data_dj4[nb_train: nb_set, 0:30]\n",
        "    y_train_dj4 = dataset_dj4[:nb_train, 30:31]\n",
        "    y_test_dj4 = dataset_dj4[nb_train: nb_set, 30:31]\n",
        "    x_train_dj4 = x_train_dj4.reshape(x_train_dj4.shape[0], x_train_dj4.shape[1], 1)\n",
        "    x_test_dj4 = x_test_dj4.reshape(x_test_dj4.shape[0], x_test_dj4.shape[1], 1)\n",
        "    input_node_dj4 = x_train_dj4.shape[1]\n",
        "    output_node_dj4 = y_train_dj4.shape[1]\n",
        "    n_timesteps_dj4 = x_train_dj4.shape[2]\n",
        "\n",
        "\n",
        "    dataframe_dj5 = pandas.read_csv(path+'DataJoin5.csv')\n",
        "    dataset_dj5 = dataframe_dj5.values\n",
        "    data_dj5 = dataset_dj5[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj5 = scaler.fit_transform(data_dj5)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj5 = data_dj5[:nb_train, 0:30]\n",
        "    x_test_dj5 = data_dj5[nb_train:nb_set, 0:30]\n",
        "    y_train_dj5 = dataset_dj5[:nb_train, 30:31]\n",
        "    y_test_dj5 = dataset_dj5[nb_train:nb_set, 30:31]\n",
        "    x_train_dj5 = x_train_dj5.reshape(x_train_dj5.shape[0], x_train_dj5.shape[1], 1)\n",
        "    x_test_dj5 = x_test_dj5.reshape(x_test_dj5.shape[0], x_test_dj5.shape[1], 1)\n",
        "    input_node_dj5 = x_train_dj5.shape[1]\n",
        "    output_node_dj5 = y_train_dj5.shape[1]\n",
        "    n_timesteps_dj5 = x_train_dj5.shape[2]\n",
        "\n",
        "    dataframe_dj6 = pandas.read_csv(path+'DataJoin6.csv')\n",
        "    dataset_dj6 = dataframe_dj6.values\n",
        "    data_dj6 = dataset_dj6[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj6 = scaler.fit_transform(data_dj6)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj6 = data_dj6[:nb_train, 0:30]\n",
        "    x_test_dj6 = data_dj6[nb_train:nb_set, 0:30]\n",
        "    y_train_dj6 = dataset_dj6[:nb_train, 30:31]\n",
        "    y_test_dj6 = dataset_dj6[nb_train:nb_set, 30:31]\n",
        "    x_train_dj6 = x_train_dj6.reshape(x_train_dj6.shape[0], x_train_dj6.shape[1], 1)\n",
        "    x_test_dj6 = x_test_dj6.reshape(x_test_dj6.shape[0], x_test_dj6.shape[1], 1)\n",
        "    input_node_dj6 = x_train_dj6.shape[1]\n",
        "    output_node_dj6 = y_train_dj6.shape[1]\n",
        "    n_timesteps_dj6 = x_train_dj6.shape[2]\n",
        "\n",
        "    dataframe_dj7 = pandas.read_csv(path+'DataJoin7.csv')\n",
        "    dataset_dj7 = dataframe_dj7.values\n",
        "    data_dj7 = dataset_dj7[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj7 = scaler.fit_transform(data_dj7)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj7 = data_dj7[:nb_train, 0:30]\n",
        "    x_test_dj7 = data_dj7[nb_train:nb_set, 0:30]\n",
        "    y_train_dj7 = dataset_dj7[:nb_train, 30:31]\n",
        "    y_test_dj7 = dataset_dj7[nb_train:nb_set, 30:31]\n",
        "    x_train_dj7 = x_train_dj7.reshape(x_train_dj7.shape[0], x_train_dj7.shape[1], 1)\n",
        "    x_test_dj7 = x_test_dj7.reshape(x_test_dj7.shape[0], x_test_dj7.shape[1], 1)\n",
        "    input_node_dj7 = x_train_dj7.shape[1]\n",
        "    output_node_dj7 = y_train_dj7.shape[1]\n",
        "    n_timesteps_dj7 = x_train_dj7.shape[2]\n",
        "\n",
        "\n",
        "    dataframe_dj8 = pandas.read_csv(path+'DataJoin8.csv')\n",
        "    dataset_dj8 = dataframe_dj8.values\n",
        "    data_dj8 = dataset_dj8[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj8 = scaler.fit_transform(data_dj8)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj8 = data_dj8[:nb_train, 0:30]\n",
        "    x_test_dj8 = data_dj8[nb_train:nb_set, 0:30]\n",
        "    y_train_dj8 = dataset_dj8[:nb_train, 30:31]\n",
        "    y_test_dj8 = dataset_dj8[nb_train:nb_set, 30:31]\n",
        "    x_train_dj8 = x_train_dj8.reshape(x_train_dj8.shape[0], x_train_dj8.shape[1], 1)\n",
        "    x_test_dj8 = x_test_dj8.reshape(x_test_dj8.shape[0], x_test_dj8.shape[1], 1)\n",
        "    input_node_dj8 = x_train_dj8.shape[1]\n",
        "    output_node_dj8 = y_train_dj8.shape[1]\n",
        "    n_timesteps_dj8 = x_train_dj8.shape[2]\n",
        "\n",
        "\n",
        "    dataframe_dj9 = pandas.read_csv(path+'DataJoin9.csv')\n",
        "    dataset_dj9 = dataframe_dj9.values\n",
        "    data_dj9 = dataset_dj9[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj9 = scaler.fit_transform(data_dj9)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj9 = data_dj9[:nb_train, 0:30]\n",
        "    x_test_dj9 = data_dj9[nb_train:nb_set, 0:30]\n",
        "    y_train_dj9 = dataset_dj9[:nb_train, 30:31]\n",
        "    y_test_dj9 = dataset_dj9[nb_train:nb_set, 30:31]\n",
        "    x_train_dj9 = x_train_dj9.reshape(x_train_dj9.shape[0], x_train_dj9.shape[1], 1)\n",
        "    x_test_dj9 = x_test_dj9.reshape(x_test_dj9.shape[0], x_test_dj9.shape[1], 1)\n",
        "    input_node_dj9 = x_train_dj9.shape[1]\n",
        "    output_node_dj9 = y_train_dj9.shape[1]\n",
        "    n_timesteps_dj9 = x_train_dj9.shape[2]\n",
        "\n",
        "    dataframe_dj10 = pandas.read_csv(path+'DataJoin10.csv')\n",
        "    dataset_dj10 = dataframe_dj10.values\n",
        "    data_dj10 = dataset_dj10[:, 0:30]\n",
        "    scaler = MinMaxScaler()\n",
        "    data_dj10 = scaler.fit_transform(data_dj10)\n",
        "    nb_set = 524288\n",
        "    # nb_set = 100\n",
        "    nb_train = int(nb_set * 0.8)\n",
        "    x_train_dj10 = data_dj10[:nb_train, 0:30]\n",
        "    x_test_dj10 = data_dj10[nb_train:nb_set, 0:30]\n",
        "    y_train_dj10 = dataset_dj10[:nb_train, 30:31]\n",
        "    y_test_dj10 = dataset_dj10[nb_train:nb_set, 30:31]\n",
        "    x_train_dj10 = x_train_dj10.reshape(x_train_dj10.shape[0], x_train_dj10.shape[1], 1)\n",
        "    x_test_dj10 = x_test_dj10.reshape(x_test_dj10.shape[0], x_test_dj10.shape[1], 1)\n",
        "    input_node_dj10 = x_train_dj10.shape[1]\n",
        "    output_node_dj10 = y_train_dj10.shape[1]\n",
        "    n_timesteps_dj10 = x_train_dj10.shape[2]\n",
        "\n",
        "\n",
        "    agrs1 = [input_node_dj1, n_timesteps_dj1, output_node_dj1, x_train_dj1, y_train_dj1,x_test_dj1, y_test_dj1]\n",
        "    agrs2 = [input_node_dj2, n_timesteps_dj2, output_node_dj2, x_train_dj2, y_train_dj2,x_test_dj2, y_test_dj2]\n",
        "    agrs3 = [input_node_dj3, n_timesteps_dj3, output_node_dj3, x_train_dj3, y_train_dj3, x_test_dj3, y_test_dj3]\n",
        "    agrs4 = [input_node_dj4, n_timesteps_dj4, output_node_dj4, x_train_dj4, y_train_dj4, x_test_dj4, y_test_dj4]\n",
        "    agrs5 = [input_node_dj5, n_timesteps_dj5, output_node_dj5, x_train_dj5, y_train_dj5, x_test_dj5, y_test_dj5]\n",
        "    agrs6 = [input_node_dj6, n_timesteps_dj6, output_node_dj6, x_train_dj6, y_train_dj6, x_test_dj6, y_test_dj6]\n",
        "    agrs7 = [input_node_dj7, n_timesteps_dj7, output_node_dj7, x_train_dj7, y_train_dj7, x_test_dj7, y_test_dj7]\n",
        "    agrs8 = [input_node_dj8, n_timesteps_dj8, output_node_dj8, x_train_dj8, y_train_dj8, x_test_dj8, y_test_dj8]\n",
        "    agrs9 = [input_node_dj9, n_timesteps_dj9, output_node_dj9, x_train_dj9, y_train_dj9, x_test_dj9, y_test_dj9]\n",
        "    agrs10 = [input_node_dj10, n_timesteps_dj10, output_node_dj10, x_train_dj10, y_train_dj10, x_test_dj10, y_test_dj10]\n",
        "\n",
        "\n",
        "    # # Parallel run with Process\n",
        "    p1 = mp.Process(target=CGRUmodel_dj1, args=agrs1)\n",
        "    p2 = mp.Process(target=CGRUmodel_dj2, args=agrs2)\n",
        "    p3 = mp.Process(target=CGRUmodel_dj3, args=agrs3) \n",
        "    p4 = mp.Process(target=CGRUmodel_dj4, args=agrs4)\n",
        "    p5 = mp.Process(target=CGRUmodel_dj5, args=agrs5)\n",
        "    p6 = mp.Process(target=CGRUmodel_dj6, args=agrs6)\n",
        "    p7 = mp.Process(target=CGRUmodel_dj7, args=agrs7)\n",
        "    p8 = mp.Process(target=CGRUmodel_dj8, args=agrs8)\n",
        "    p9 = mp.Process(target=CGRUmodel_dj9, args=agrs9)\n",
        "    p10 = mp.Process(target=CGRUmodel_dj10, args=agrs10)\n",
        "\n",
        "\n",
        "    p1.start()\n",
        "    p2.start()\n",
        "    p3.start()\n",
        "    p4.start()\n",
        "    p5.start()\n",
        "    p6.start()\n",
        "    p7.start()\n",
        "    p8.start()\n",
        "    p9.start()\n",
        "    p10.start()\n",
        "\n",
        "\n",
        "\n",
        "    p1.join()\n",
        "    p2.join()\n",
        "    p3.join()\n",
        "    p4.join()\n",
        "    p5.join()\n",
        "    p6.join()\n",
        "    p7.join()\n",
        "    p8.join()\n",
        "    p9.join()\n",
        "    p10.join()\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    Time = stop - start\n",
        "    print('Time = ', Time)\n",
        "\n",
        "    f = open('/content/drive/My Drive/Parallel_10Joints_128/Total_Computational_Time.txt', 'w')\n",
        "    s = str(Time)\n",
        "    f.write(s)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "sfs_2QGl6LAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}